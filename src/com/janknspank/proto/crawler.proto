// Objects representing crawler instructions.

import "com/janknspank/database/extensions.proto";

option java_package = "com.janknspank.proto";
option java_outer_classname = "CrawlerProto";

// Defines crawling instructions for a news site.  The minimal set of crawling
// instructions is a root domain, a start URL (usually the http:// of the
// homepage), a regular expression set for determining which URLs represent
// articles, a set of CSS element selectors for specifying where the paragraph
// elements are, and test instructions containing 5 article URLs and 5
// non-Article URLs that validate the regular expression set.
//
// Additional instructions are allowed and encouraged.  You should blacklist
// subdomains and paths that are not literary or news-oriented, such as job
// boards, conference promotions, stock market quotes, UGC forums, etc.  And
// adding test instructions for these blacklists is extremely helpful for
// debugging and explaining what's being blacklisted.

message SiteManifest {
  // Primary identifier for this site.  DO NOT INCLUDE "www.".
  // E.g. "nytimes.com".
  optional string root_domain = 1 [
    (required) = YES,
    (string_length) = 767
  ];

  // Other domains that this site goes by.  Often used for International
  // editions.  E.g. bbc.com, which is the US-site for bbc.co.uk.
  // These should not be subdomains of the root domain... That would just be
  // redundant.
  repeated string aka_root_domain = 2 [
    (string_length) = 767
  ];

  // This is where we'll start the crawlers, so specify any good start URLs:
  // Usually the home page is good, but also interesting subdomains and paths
  // for subsites are very helpful to know about.
  // E.g. "https://www.nytimes.com/", "http://sf.curbed.com".
  repeated string start_url = 3 [
    (required) = YES,
    (string_length) = 767
  ];

  // Any subdomains listed here will not be crawled.  Please list anything that
  // would be non-news oriented, especially authentication, UGC forums,
  // conference information sites, job boards, real estate, stock quotes...
  repeated string subdomain_blacklist = 4 [
    (string_length) = 767
  ];

  // Any paths blacklisted here will not be crawled.
  message PathBlacklist {
    // A string or regular expression to blacklist.
    optional string needle = 1 [
      (required) = YES,
      (string_length) = 767
    ];

    // For strings, where to look for the string.
    enum Location {
      // The path must perfectly equal the needle to be blacklisted.
      EQUALS = 1;
      // The path must start with the needle to be blacklisted.
      STARTS_WITH = 2;
      // The path must end with the needle to be blacklisted.
      ENDS_WITH = 3;
      // The path must contain the needle to be blacklisted.
      CONTAINS = 4;
      // If a substring in the path matches this regular expression, it's
      // blacklisted.
      REGEX_FIND = 5;
      // If the entire path matches this regular expression, it's blacklisted.
      REGEX_MATCH = 6;
    }
    optional Location location = 4 [
      default = STARTS_WITH
    ];
  }
  repeated PathBlacklist path_blacklist = 5;

  // Any DocumentNode children for articles from this website that match this
  // CSS-style element selector are considered paragraphs.
  repeated string paragraph_selector = 6 [
    (required) = YES,
    (string_length) = 767
  ];

  // For all Nodes that match paragraph_selector, if they or any of their
  // children also match this this selector, they are excluded as paragraphs.
  repeated string paragraph_blacklist_selector = 14 [
    (string_length) = 767
  ];

  // A collection of regular expressions that together identify all article URLs
  // on this site.  If any Pattern matches, the URL is determined to be an
  // article.
  message ArticleUrlPattern {
    // If present, a subdomain that this regular expression is restricted to.
    optional string subdomain = 1 [
      (string_length) = 767
    ];

    // A regular expression specification to run against the article's path.
    optional string path_regex = 2 [
      (string_length) = 767
    ];

    // A regular expression specification to run against the article unparsed
    // query string.
    optional string query_regex = 3 [
      (string_length) = 767
    ];
  }
  repeated ArticleUrlPattern article_url_pattern = 7;

  // These are rare: URL query parameters that are actually used to address a
  // unique article.  Any tracking query parameters or UI-configuring query
  // parameters should NOT be listed.  Basically, only put article ID specifiers
  // here, IFF the respective site uses query parameters to address articles.
  repeated string whitelisted_query_parameter = 8 [
    (string_length) = 767
  ];

  // An RSS or Atom URL for getting URLs hosted on this content site.
  repeated string rss_url = 9 [
    (string_length) = 767
  ];

  optional TestInstructions test_instructions = 10 [
    (required) = YES
  ];

  optional bool is_https = 11;

  // If a site has a special place in the DOM to put a clean non-cluttered-up-
  // with-annotations title, this lets manifests explain where that is!
  repeated string title_selector = 12 [
    (string_length) = 767
  ];

  // URL patterns that indicate whether articles should have their relevancy
  // for features boosted due to their domain, subdomain, path, or query
  // parameters.  E.g. articles in rigzone.com should receive a boost for being
  // Oil & Energy articles.  In this case, no subdomain, path_regex, or
  // query_regex would be set.  In other cases, certain paths on nytimes.com
  // could be used to boost feature scores.
  message FeatureBoostPattern {
    // If present, a subdomain that this regular expression is restricted to.
    optional string subdomain = 1 [
      (string_length) = 767
    ];

    // A regular expression specification to run against the article's path.
    optional string path_regex = 2 [
      (string_length) = 767
    ];

    // A regular expression specification to run against the article unparsed
    // query string.
    optional string query_regex = 3 [
      (string_length) = 767
    ];

    optional int32 feature_id = 4;

    // A number between -20 and 10 that indicates the strength of the boost that
    // should be applied.  -20 is an incredible punishment, 10 is a very strong
    // boost.  0 is what most articles should get if we don't know what topic
    // they're about and they're on a generic web site.
    optional int32 boost = 5 [
      (required) = YES
    ];
  }
  repeated FeatureBoostPattern feature_boost_pattern = 13;
}

// Any URLs in these lists will be checked as either whitelisted or not, or
// articles or not, depending which list you add them to.  These are a GREAT
// HELP TO YOU to validate your regular expressions and blacklists, please
// do use them!!
message TestInstructions {
  message UrlWhitelistChecks {
    repeated string good_url = 1 [
      (string_length) = 767
    ];
    repeated string bad_url = 2 [
      (string_length) = 767
    ];
  }
  optional UrlWhitelistChecks url_whitelist_checks = 1;

  message ArticleUrlDetectorChecks {
    repeated string article_url = 1 [
      (string_length) = 767
    ];
    repeated string non_article_url = 2 [
      (string_length) = 767
    ];
  }
  optional ArticleUrlDetectorChecks article_url_detector_checks = 2 [
    (required) = YES
  ];
}

// Records a run of ArticleCrawler: What sites did it crawl, how long did each
// take, how did the process end, etc.
message CrawlHistory {
  option (database_collection) = "MongoDB.CrawlHistory";

  optional string crawl_id = 1 [
    (required) = YES,
    (storage_method) = PRIMARY_KEY,
    (string_length) = 24,
    (string_charset) = LATIN1
  ];

  optional string host = 2 [
    (required) = YES,
    (string_length) = 256
  ];

  message Site {
    optional string root_domain = 1 [
      (required) = YES,
      (string_length) = 767
    ];

    optional int64 start_time = 2 [
      (required) = YES
    ];

    optional int64 end_time = 3;

    // Elapsed time.
    optional int64 millis = 4;

    optional int32 articles_crawled = 5 [
      (required) = YES
    ];
  }
  repeated Site site = 3;

  optional int64 start_time = 4 [
    (required) = YES
  ];

  optional int64 end_time = 5;

  // Elapsed time.
  optional int64 millis = 6;

  optional bool was_interrupted = 7;
}
